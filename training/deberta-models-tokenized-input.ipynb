{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:36.975550Z",
     "start_time": "2025-05-13T14:05:32.082884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DebertaV2Tokenizer,\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback, DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from utils import SPECIAL_TOKENS"
   ],
   "id": "720e1aff3e0ca141",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.060982Z",
     "start_time": "2025-05-13T14:05:37.056645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)"
   ],
   "id": "2a5ae0d66448aa31",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.083516Z",
     "start_time": "2025-05-13T14:05:37.067179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "d661f12e6e57c12f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.099412Z",
     "start_time": "2025-05-13T14:05:37.097829Z"
    }
   },
   "cell_type": "code",
   "source": "project_root = os.path.abspath(\"..\")",
   "id": "7f3ff76271dcb86f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.123515Z",
     "start_time": "2025-05-13T14:05:37.108368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = os.path.join(project_root, \"datasets/processed_dataset.csv\")\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "dataset = Dataset.from_pandas(df)"
   ],
   "id": "a57a80d69c642697",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.131520Z",
     "start_time": "2025-05-13T14:05:37.130006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenized(example):\n",
    "    return tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=True\n",
    "    )"
   ],
   "id": "702fd19727e834b8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.139358Z",
     "start_time": "2025-05-13T14:05:37.137689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BEST_MODELS_PATH = \"best_models.json\"\n",
    "F1_TOLERANCE = 1e-2\n",
    "\n",
    "def load_best_models():\n",
    "    if not os.path.exists(BEST_MODELS_PATH):\n",
    "        return {}\n",
    "    with open(BEST_MODELS_PATH, \"r\") as f:\n",
    "        return json.load(f)\n"
   ],
   "id": "bdbcaf8c606b5124",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.146707Z",
     "start_time": "2025-05-13T14:05:37.144453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_best_model(model_name, f1_score, precision, recall, accuracy):\n",
    "    best_models = load_best_models()\n",
    "    current_best = best_models.get(model_name, {\"f1\": 0.0, \"precision\": 0.0})\n",
    "\n",
    "    better_f1 = f1_score - F1_TOLERANCE > current_best[\"f1\"]\n",
    "    similar_f1 = abs(f1_score - current_best[\"f1\"]) <= F1_TOLERANCE\n",
    "    better_precision = precision > current_best[\"precision\"]\n",
    "\n",
    "    if better_f1 or (similar_f1 and better_precision):\n",
    "        print(f\"üéØ New best for {model_name}!\")\n",
    "        best_models[model_name] = {\n",
    "            \"f1\": f1_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "        with open(BEST_MODELS_PATH, \"w\") as f:\n",
    "            json.dump(best_models, f, indent=2)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"üß™ {model_name} did not improve:\")\n",
    "        print(f\"F1: {f1_score:.4f} (best: {current_best['f1']:.4f}) | Precision: {precision:.4f} (best: {current_best['precision']:.4f})\")\n",
    "        return False\n"
   ],
   "id": "ffe7d84c6f5f63fd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:05:37.153510Z",
     "start_time": "2025-05-13T14:05:37.151939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ],
   "id": "9b08b8d565bfb14d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:31:11.047090Z",
     "start_time": "2025-05-13T14:05:37.158472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODELS = [\"microsoft/deberta-v3-small\", \"microsoft/deberta-v3-base\", \"microsoft/deberta-base\"]\n",
    "results = {}\n",
    "best_f1 = -1\n",
    "best_model_name = None\n",
    "\n",
    "for model_id in MODELS:\n",
    "    model_tag = model_id.replace(\"/\", \"-\").replace(\"_\", \"-\")\n",
    "    model_name = f\"{model_tag}-tokenized\"\n",
    "    print(f\"\\nüöÄ Training model: {model_name}\")\n",
    "\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenized, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.25, seed=42)\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"])\n",
    "\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_name}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.0,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        logging_dir=f\"./logs_{model_name}\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        optim=\"adamw_torch\",\n",
    "        adam_epsilon=1e-8\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    results[model_name] = eval_result\n",
    "\n",
    "    if update_best_model(\n",
    "        model_name=f\"{model_name}\",\n",
    "        f1_score=eval_result[\"eval_f1\"],\n",
    "        precision=eval_result[\"eval_precision\"],\n",
    "        recall=eval_result[\"eval_recall\"],\n",
    "        accuracy=eval_result[\"eval_accuracy\"]\n",
    "    ):\n",
    "        trainer.save_model(f\"models/best-{model_name}\")\n",
    "\n",
    "    if eval_result[\"eval_f1\"] > best_f1:\n",
    "        best_f1 = eval_result[\"eval_f1\"]\n",
    "        best_model_name = model_name\n",
    "\n",
    "# ‚úÖ Summary\n",
    "print(\"\\n--- Summary of Results ---\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}: F1 = {result['eval_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best model: {best_model_name} (F1 = {best_f1:.4f})\")"
   ],
   "id": "408e327dc5520118",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training model: microsoft-deberta-v3-small-tokenized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd27b6b7ee67470d9b727e65a17c4063"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1eac34efb812477d817e95b3ca5c3936"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec1f03b3fcb549019cdc426c8f05c376"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ef5d7d020524b3380189c594e43ba44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "644a117a9c1741a8bc9fa17476dc1264"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/var/folders/vz/ddck6jjd6m90zdsvp6wmk4qm0000gn/T/ipykernel_57741/1450641794.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9065198221ee4385b8e1b766143ce69e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [204/510 04:05 < 06:11, 0.82 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587997</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.634328</td>\n",
       "      <td>0.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.590947</td>\n",
       "      <td>0.692593</td>\n",
       "      <td>0.652695</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.724252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.723881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.569133</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:05]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ New best for microsoft-deberta-v3-small-tokenized!\n",
      "\n",
      "üöÄ Training model: microsoft-deberta-v3-base-tokenized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49929a9128e4468b9af6691ddff0f845"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/vz/ddck6jjd6m90zdsvp6wmk4qm0000gn/T/ipykernel_57741/1450641794.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/510 20:29 < 05:08, 0.33 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.620152</td>\n",
       "      <td>0.714815</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.683128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.579499</td>\n",
       "      <td>0.707407</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.694981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640787</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.776596</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>0.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541733</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.724409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.556673</td>\n",
       "      <td>0.737037</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.730038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.551947</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.744361</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.741573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.582096</td>\n",
       "      <td>0.714815</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.574513</td>\n",
       "      <td>0.707407</td>\n",
       "      <td>0.706767</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.704120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:11]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ New best for microsoft-deberta-v3-base-tokenized!\n",
      "\n",
      "üöÄ Training model: microsoft-deberta-base-tokenized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f00098e74f54335ae968f1a87f16c95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f1a318075a14065b9bbbd38b39e8279"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DebertaTokenizer'. \n",
      "The class this function is called from is 'DebertaV2Tokenizer'.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2302\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2301\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2302\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39minit_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minit_kwargs)\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2.py:101\u001B[0m, in \u001B[0;36mDebertaV2Tokenizer.__init__\u001B[0;34m(self, vocab_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, sp_model_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model_kwargs \u001B[38;5;241m=\u001B[39m {} \u001B[38;5;28;01mif\u001B[39;00m sp_model_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m sp_model_kwargs\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(vocab_file):\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a vocabulary file at path \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvocab_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. To load the vocabulary from a Google pretrained\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model use `tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    105\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/genericpath.py:30\u001B[0m, in \u001B[0;36misfile\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     st \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mstat(path)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "\u001B[0;31mTypeError\u001B[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_tag\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-tokenized\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müöÄ Training model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m DebertaV2Tokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n\u001B[1;32m     12\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39madd_special_tokens({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madditional_special_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: SPECIAL_TOKENS})\n\u001B[1;32m     14\u001B[0m tokenized_dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mmap(tokenized, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2062\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2059\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2060\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2062\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_from_pretrained(\n\u001B[1;32m   2063\u001B[0m     resolved_vocab_files,\n\u001B[1;32m   2064\u001B[0m     pretrained_model_name_or_path,\n\u001B[1;32m   2065\u001B[0m     init_configuration,\n\u001B[1;32m   2066\u001B[0m     \u001B[38;5;241m*\u001B[39minit_inputs,\n\u001B[1;32m   2067\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m   2068\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m   2069\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m   2070\u001B[0m     _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[1;32m   2071\u001B[0m     _is_local\u001B[38;5;241m=\u001B[39mis_local,\n\u001B[1;32m   2072\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[1;32m   2073\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2074\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2303\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2301\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2302\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39minit_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minit_kwargs)\n\u001B[0;32m-> 2303\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n\u001B[1;32m   2304\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m   2305\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2307\u001B[0m     )\n\u001B[1;32m   2308\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:87\u001B[0m, in \u001B[0;36mimport_protobuf_decode_error\u001B[0;34m(error_message)\u001B[0m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DecodeError\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PROTOBUF_IMPORT_ERROR\u001B[38;5;241m.\u001B[39mformat(error_message))\n",
      "\u001B[0;31mImportError\u001B[0m: \n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
