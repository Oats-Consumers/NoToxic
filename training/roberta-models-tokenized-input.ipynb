{
 "cells": [
  {
   "cell_type": "code",
   "id": "720e1aff3e0ca141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.435081Z",
     "start_time": "2025-05-16T09:41:38.433049Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from utils import SPECIAL_TOKENS\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "d661f12e6e57c12f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.451437Z",
     "start_time": "2025-05-16T09:41:38.449235Z"
    }
   },
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "2f016cb8-6e95-4105-b6f3-95353eabc0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.470585Z",
     "start_time": "2025-05-16T09:41:38.468336Z"
    }
   },
   "source": [
    "# Ensure consistent results\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7f3ff76271dcb86f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.478853Z",
     "start_time": "2025-05-16T09:41:38.477467Z"
    }
   },
   "source": "project_root = os.path.abspath(\"..\")",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a57a80d69c642697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.540418Z",
     "start_time": "2025-05-16T09:41:38.485322Z"
    }
   },
   "source": [
    "DATASET_PATH = os.path.join(project_root, \"datasets/processed_dataset.csv\")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=SEED)\n",
    "\n",
    "# Convert to HF Dataset\n",
    "dataset = {\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.549643Z",
     "start_time": "2025-05-16T09:41:38.547322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = {\n",
    "    \"train\": Counter(train_df['label']),\n",
    "    \"validation\": Counter(val_df['label']),\n",
    "    \"test\": Counter(test_df['label'])\n",
    "}\n",
    "\n",
    "\n",
    "for name, batch in label_counts.items():\n",
    "    print(name)\n",
    "    for label, count in batch.items():\n",
    "        total = sum(batch.values())\n",
    "        print(f\"Label {label}: {count} samples ({count/total:.2%})\")"
   ],
   "id": "163ce2a90f27dd90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Label 0: 3900 samples (46.41%)\n",
      "Label 1: 4504 samples (53.59%)\n",
      "validation\n",
      "Label 1: 563 samples (53.62%)\n",
      "Label 0: 487 samples (46.38%)\n",
      "test\n",
      "Label 0: 488 samples (46.43%)\n",
      "Label 1: 563 samples (53.57%)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.562513Z",
     "start_time": "2025-05-16T09:41:38.560142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = {\n",
    "    \"train\": Counter(train_df['label']),\n",
    "    \"validation\": Counter(val_df['label']),\n",
    "    \"test\": Counter(test_df['label'])\n",
    "}\n",
    "\n",
    "\n",
    "for name, batch in label_counts.items():\n",
    "    print(name)\n",
    "    for label, count in batch.items():\n",
    "        total = sum(batch.values())\n",
    "        print(f\"Label {label}: {count} samples ({count/total:.2%})\")"
   ],
   "id": "916a78d8a21817ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Label 0: 3900 samples (46.41%)\n",
      "Label 1: 4504 samples (53.59%)\n",
      "validation\n",
      "Label 1: 563 samples (53.62%)\n",
      "Label 0: 487 samples (46.38%)\n",
      "test\n",
      "Label 0: 488 samples (46.43%)\n",
      "Label 1: 563 samples (53.57%)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "702fd19727e834b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.571674Z",
     "start_time": "2025-05-16T09:41:38.570231Z"
    }
   },
   "source": [
    "def tokenized(example):\n",
    "    return tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=True\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "bdbcaf8c606b5124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.580264Z",
     "start_time": "2025-05-16T09:41:38.578730Z"
    }
   },
   "source": [
    "BEST_MODELS_PATH = \"best_models.json\"\n",
    "F1_TOLERANCE = 1e-3\n",
    "\n",
    "def load_best_models():\n",
    "    if not os.path.exists(BEST_MODELS_PATH):\n",
    "        return {}\n",
    "    with open(BEST_MODELS_PATH, \"r\") as f:\n",
    "        return json.load(f)\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "ffe7d84c6f5f63fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.588451Z",
     "start_time": "2025-05-16T09:41:38.586254Z"
    }
   },
   "source": [
    "def update_best_model(model_name, f1_score, precision, recall, accuracy):\n",
    "    best_models = load_best_models()\n",
    "    current_best = best_models.get(model_name, {\"f1\": 0.0, \"precision\": 0.0})\n",
    "\n",
    "    better_f1 = f1_score - F1_TOLERANCE > current_best[\"f1\"]\n",
    "    similar_f1 = abs(f1_score - current_best[\"f1\"]) <= F1_TOLERANCE\n",
    "    better_precision = precision > current_best[\"precision\"]\n",
    "\n",
    "    if better_f1 or (similar_f1 and better_precision):\n",
    "        print(f\"🎯 New best for {model_name}!\")\n",
    "        best_models[model_name] = {\n",
    "            \"f1\": f1_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "        with open(BEST_MODELS_PATH, \"w\") as f:\n",
    "            json.dump(best_models, f, indent=2)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"🧪 {model_name} did not improve:\")\n",
    "        print(f\"F1: {f1_score:.4f} (best: {current_best['f1']:.4f}) | Precision: {precision:.4f} (best: {current_best['precision']:.4f})\")\n",
    "        return False\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "9b08b8d565bfb14d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.596103Z",
     "start_time": "2025-05-16T09:41:38.594579Z"
    }
   },
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "4f42e497-75e9-4a90-a708-a7d8dff8b7a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:38.605269Z",
     "start_time": "2025-05-16T09:41:38.602685Z"
    }
   },
   "source": [
    "def get_training_args(model_id, seed=SEED):\n",
    "    return {\n",
    "        \"roberta-base\": TrainingArguments(\n",
    "            output_dir=f\"./results_{model_id}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=32,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=10,\n",
    "            weight_decay=0.01,\n",
    "            warmup_ratio=0.06,\n",
    "            logging_dir=f\"./logs_{model_id}\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            save_total_limit=1,\n",
    "            optim=\"adamw_torch\",\n",
    "            adam_epsilon=1e-8,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=seed,\n",
    "            fp16=True,\n",
    "        ),\n",
    "        \"roberta-large\": TrainingArguments(\n",
    "            output_dir=f\"./results_{model_id}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=1e-5,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4, \n",
    "            num_train_epochs=8,\n",
    "            weight_decay=0.01,\n",
    "            warmup_ratio=0.1,\n",
    "            logging_dir=f\"./logs_{model_id}\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            save_total_limit=1,\n",
    "            optim=\"adamw_torch\",\n",
    "            adam_epsilon=1e-8,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=seed,\n",
    "            fp16=True,\n",
    "        ),\n",
    "        \"s-nlp/roberta_toxicity_classifier\": TrainingArguments(\n",
    "            output_dir=f\"./results_{model_id.replace('/', '-')}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=32,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=10,\n",
    "            weight_decay=0.01,\n",
    "            warmup_ratio=0.06,\n",
    "            logging_dir=f\"./logs_{model_id.replace('/', '-')}\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            save_total_limit=1,\n",
    "            optim=\"adamw_torch\",\n",
    "            adam_epsilon=1e-8,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=seed,\n",
    "            fp16=True,\n",
    "        ),\n",
    "    }[model_id]"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "408e327dc5520118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T09:41:42.404419Z",
     "start_time": "2025-05-16T09:41:38.611136Z"
    }
   },
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "MODELS = [\n",
    "    \"roberta-large\",\n",
    "    \"roberta-base\", \n",
    "    \"s-nlp/roberta_toxicity_classifier\", \n",
    "]\n",
    "\n",
    "results = {}\n",
    "best_f1 = -1\n",
    "best_model_name = None\n",
    "\n",
    "for model_id in MODELS:\n",
    "    model_tag = model_id.replace(\"/\", \"-\").replace(\"_\", \"-\")\n",
    "    model_name = f\"{model_tag}-tokenized\"\n",
    "    print(f\"\\n🚀 Training model: {model_name}\")\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "\n",
    "    tokenized_dataset = {\n",
    "        k: v.map(tokenized, batched=True) for k, v in dataset.items()\n",
    "    }\n",
    "    for split in tokenized_dataset:\n",
    "        tokenized_dataset[split].set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"]\n",
    "        )\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = get_training_args(model_id)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    print(eval_result[\"eval_f1\"], eval_result[\"eval_precision\"])\n",
    "    results[model_name] = eval_result\n",
    "\n",
    "    if update_best_model(\n",
    "        model_name=f\"{model_name}\",\n",
    "        f1_score=eval_result[\"eval_f1\"],\n",
    "        precision=eval_result[\"eval_precision\"],\n",
    "        recall=eval_result[\"eval_recall\"],\n",
    "        accuracy=eval_result[\"eval_accuracy\"]\n",
    "    ):\n",
    "        trainer.save_model(f\"models/best-{model_name}\")\n",
    "\n",
    "    if eval_result[\"eval_f1\"] > best_f1:\n",
    "        best_f1 = eval_result[\"eval_f1\"]\n",
    "        best_model_name = model_name\n",
    "\n",
    "# ✅ Summary\n",
    "print(\"\\n--- Summary of Results ---\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}: F1 = {result['eval_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 Best model: {best_model_name} (F1 = {best_f1:.4f})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training model: roberta-large-tokenized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/8404 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bff43f216ca241af93dc6056f6b6cd38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 22\u001B[0m\n\u001B[1;32m     18\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m RobertaTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n\u001B[1;32m     19\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39madd_special_tokens({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madditional_special_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: SPECIAL_TOKENS})\n\u001B[1;32m     21\u001B[0m tokenized_dataset \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m---> 22\u001B[0m     k: v\u001B[38;5;241m.\u001B[39mmap(tokenized, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     23\u001B[0m }\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m tokenized_dataset:\n\u001B[1;32m     25\u001B[0m     tokenized_dataset[split]\u001B[38;5;241m.\u001B[39mset_format(\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     27\u001B[0m         columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     28\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    555\u001B[0m }\n\u001B[1;32m    556\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 557\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    558\u001B[0m datasets: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/datasets/arrow_dataset.py:3074\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3069\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[1;32m   3070\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3071\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3072\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3073\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3074\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[1;32m   3075\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   3076\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/datasets/arrow_dataset.py:3516\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[1;32m   3514\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3515\u001B[0m     _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m-> 3516\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, batch \u001B[38;5;129;01min\u001B[39;00m iter_outputs(shard_iterable):\n\u001B[1;32m   3517\u001B[0m         num_examples_in_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(i)\n\u001B[1;32m   3518\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m update_data:\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/datasets/arrow_dataset.py:3466\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.iter_outputs\u001B[0;34m(shard_iterable)\u001B[0m\n\u001B[1;32m   3464\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3465\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[0;32m-> 3466\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m i, apply_function(example, i, offset\u001B[38;5;241m=\u001B[39moffset)\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/datasets/arrow_dataset.py:3389\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function\u001B[0;34m(pa_inputs, indices, offset)\u001B[0m\n\u001B[1;32m   3387\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001B[39;00m\n\u001B[1;32m   3388\u001B[0m inputs, fn_args, additional_args, fn_kwargs \u001B[38;5;241m=\u001B[39m prepare_inputs(pa_inputs, indices, offset\u001B[38;5;241m=\u001B[39moffset)\n\u001B[0;32m-> 3389\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[1;32m   3390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001B[0;32mIn[21], line 2\u001B[0m, in \u001B[0;36mtokenized\u001B[0;34m(example)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenized\u001B[39m(example):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer(\n\u001B[1;32m      3\u001B[0m         example[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_text\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      4\u001B[0m         padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m         truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      6\u001B[0m         max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m,\n\u001B[1;32m      7\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2887\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2885\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   2886\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 2887\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_one(text\u001B[38;5;241m=\u001B[39mtext, text_pair\u001B[38;5;241m=\u001B[39mtext_pair, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mall_kwargs)\n\u001B[1;32m   2888\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2889\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2975\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   2970\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2971\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2972\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2973\u001B[0m         )\n\u001B[1;32m   2974\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[0;32m-> 2975\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[1;32m   2976\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   2977\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   2978\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   2979\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   2980\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   2981\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m   2982\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m   2983\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m   2984\u001B[0m         padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m   2985\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m   2986\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m   2987\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m   2988\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m   2989\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m   2990\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m   2991\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m   2992\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m   2993\u001B[0m         split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[1;32m   2994\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2995\u001B[0m     )\n\u001B[1;32m   2996\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2997\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   2998\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   2999\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3017\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3018\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3177\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3167\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   3168\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   3169\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3170\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3174\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3175\u001B[0m )\n\u001B[0;32m-> 3177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[1;32m   3178\u001B[0m     batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   3179\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   3180\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m   3181\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[1;32m   3182\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   3183\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m   3184\u001B[0m     is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m   3185\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m   3186\u001B[0m     padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m   3187\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m   3188\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m   3189\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m   3190\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m   3191\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m   3192\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m   3193\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m   3194\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m   3195\u001B[0m     split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[1;32m   3196\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3197\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils.py:887\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    885\u001B[0m     ids, pair_ids \u001B[38;5;241m=\u001B[39m ids_or_pair_ids\n\u001B[0;32m--> 887\u001B[0m first_ids \u001B[38;5;241m=\u001B[39m get_input_ids(ids)\n\u001B[1;32m    888\u001B[0m second_ids \u001B[38;5;241m=\u001B[39m get_input_ids(pair_ids) \u001B[38;5;28;01mif\u001B[39;00m pair_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    889\u001B[0m input_ids\u001B[38;5;241m.\u001B[39mappend((first_ids, second_ids))\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils.py:854\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_input_ids\u001B[39m(text):\n\u001B[1;32m    853\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 854\u001B[0m         tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenize(text, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    855\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(tokens)\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/tokenization_utils.py:697\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.tokenize\u001B[0;34m(self, text, **kwargs)\u001B[0m\n\u001B[1;32m    695\u001B[0m         tokenized_text\u001B[38;5;241m.\u001B[39mappend(token)\n\u001B[1;32m    696\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 697\u001B[0m         tokenized_text\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenize(token))\n\u001B[1;32m    698\u001B[0m \u001B[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001B[39;00m\n\u001B[1;32m    699\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_text\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/transformers/models/roberta/tokenization_roberta.py:270\u001B[0m, in \u001B[0;36mRobertaTokenizer._tokenize\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Tokenize a string.\"\"\"\u001B[39;00m\n\u001B[1;32m    269\u001B[0m bpe_tokens \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 270\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpat, text):\n\u001B[1;32m    271\u001B[0m     token \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m    272\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbyte_encoder[b] \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m token\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    273\u001B[0m     )  \u001B[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001B[39;00m\n\u001B[1;32m    274\u001B[0m     bpe_tokens\u001B[38;5;241m.\u001B[39mextend(bpe_token \u001B[38;5;28;01mfor\u001B[39;00m bpe_token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbpe(token)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/regex/regex.py:337\u001B[0m, in \u001B[0;36mfindall\u001B[0;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001B[0m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfindall\u001B[39m(pattern, string, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, pos\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, endpos\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, overlapped\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    332\u001B[0m   concurrent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, ignore_unused\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a list of all matches in the string. The matches may be overlapped\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    if overlapped is True. If one or more groups are present in the pattern,\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;124;03m    return a list of groups; this will be a list of tuples if the pattern has\u001B[39;00m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m    more than one group. Empty matches are included in the result.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m     pat \u001B[38;5;241m=\u001B[39m _compile(pattern, flags, ignore_unused, kwargs, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pat\u001B[38;5;241m.\u001B[39mfindall(string, pos, endpos, overlapped, concurrent, timeout)\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/site-packages/regex/regex.py:466\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(pattern, flags, ignore_unused, kwargs, cache_it)\u001B[0m\n\u001B[1;32m    463\u001B[0m locale_key \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mtype\u001B[39m(pattern), pattern)\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _locale_sensitive\u001B[38;5;241m.\u001B[39mget(locale_key, \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m (flags \u001B[38;5;241m&\u001B[39m LOCALE) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;66;03m# This pattern is, or might be, locale-sensitive.\u001B[39;00m\n\u001B[0;32m--> 466\u001B[0m     pattern_locale \u001B[38;5;241m=\u001B[39m _getpreferredencoding()\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;66;03m# This pattern is definitely not locale-sensitive.\u001B[39;00m\n\u001B[1;32m    469\u001B[0m     pattern_locale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/locale.py:690\u001B[0m, in \u001B[0;36mgetpreferredencoding\u001B[0;34m(do_setlocale)\u001B[0m\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m getencoding()\n\u001B[1;32m    689\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m--> 690\u001B[0m     setlocale(LC_CTYPE, old_loc)\n",
      "File \u001B[0;32m~/anaconda3/envs/Project_Toxic_Chat_Detector/lib/python3.12/locale.py:600\u001B[0m, in \u001B[0;36msetlocale\u001B[0;34m(category, locale)\u001B[0m\n\u001B[1;32m    597\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory LC_ALL is not supported\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    598\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _parse_localename(localename)\n\u001B[0;32m--> 600\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msetlocale\u001B[39m(category, locale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    602\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Set the locale for the given category.  The locale can be\u001B[39;00m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03m        a string, an iterable of two strings (language code and encoding),\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;124;03m        or None.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    610\u001B[0m \n\u001B[1;32m    611\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    612\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m locale \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(locale, _builtin_str):\n\u001B[1;32m    613\u001B[0m         \u001B[38;5;66;03m# convert to string\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9137f1-b96e-4b69-be8f-d4685dab70a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
